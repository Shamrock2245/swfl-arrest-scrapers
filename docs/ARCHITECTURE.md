# Architecture

## Purpose
Provide a clear, up‑to‑date view of the end‑to‑end data pipeline that scrapes arrest records from Southwest Florida counties, normalizes them to a unified 34‑field schema, scores leads, and writes the results to Google Sheets. The architecture also supports staging via Apps Script and future SignNow integration.

## High‑Level Flow
1. **Scrape (per county)** – Headless browsers (Puppeteer/Playwright) fetch list/detail pages, CSV, XLS, or PDF files. Rate‑limited requests, exponential back‑off, and Cloudflare/CAPTCHA handling are built in.
2. **Extract & Normalize** – Raw data is parsed into JavaScript/Python objects and passed through `normalizers/normalize34.js` (or the Python equivalent) to produce a record that matches the unified schema defined in `SCHEMA.md`. The `LeadScorer` computes a `Lead_Score` and `Lead_Status`.
3. **Write & Mirror** – `writers/sheets34.js` (Node) and `python_scrapers/writers/sheets_writer.py` (Python) upsert records into the master Google Sheet using the composite key `(County, Booking_Number)`. Qualified leads (`Lead_Score ≥ 70`) are mirrored to the `Qualified_Arrests` tab.
4. **Post‑process** – The `updateBondPaid` job refreshes bond‑paid status for the last 14 days. Slack notifications summarize each run and surface errors.
5. **Staging & SignNow (future)** – The Apps Script UI (`Form.html`) allows operators to edit a record, then trigger a SignNow document generation. Generated PDFs are stored in Google Drive and the sheet is updated with `packet_status`, `packet_url`, and `signed_at`.

## Repository Layout
```
swfl-arrest-scrapers/
├─ scrapers/                # Node.js county scrapers
├─ python_scrapers/        # Python solvers & runners
│   ├─ scrapers/
│   ├─ writers/
│   └─ scoring/
├─ normalizers/            # Unified schema mapping
├─ writers/                # Node.js Google Sheets writer (34‑column)
├─ jobs/                   # Orchestration scripts (runAll, updateBondPaid)
├─ config/                 # schema.json, counties.json
├─ shared/                 # Browser helpers, retry logic, CAPTCHA detection
├─ fixtures/               # Saved HTML/CSV for regression tests
└─ docs/                   # Project documentation
```

## Data Contracts
- **Input** – Public county websites (HTML tables, CSV, XLS, PDF). Each county scraper normalizes its own fields.
- **Unified Schema** – 34 columns defined in `SCHEMA.md` (including `Scrape_Timestamp`).
- **Upsert Key** – `(County, Booking_Number)` ensures idempotent writes.
- **Qualified Rule** – `Lead_Score ≥ 70` marks a record as qualified.

## Idempotency & Reliability
- Idempotent upserts prevent duplicate rows.
- Exponential back‑off with jitter for network failures.
- Per‑origin concurrency limits to avoid throttling.
- Fixture‑based unit tests catch selector drift.

## Interfaces
- **Google Sheets** – Source of truth and dashboard for qualified leads.
- **Apps Script `Form.html`** – Staging UI for manual corrections and SignNow trigger.
- **SignNow** – Planned integration for e‑signature document generation.
- **Google Drive** – Stores raw extracts and final PDFs.
- **Slack** – Real‑time alerts for successes, failures, and CAPTCHAs.

## Master Google Sheet
All data is written to the sheet identified by the following URL (the single source of truth for the project):
https://docs.google.com/spreadsheets/d/10mphJQkWlDoscDoY8CGFPt96yzoB7rAbDTRrR02orUY/edit

---
*Generated by Antigravity AI assistant.*
