name: Scrape Palm Beach County

on:
  schedule:
    - cron: '45 * * * *'  # Run at minute 45 of every hour
  workflow_dispatch:      # Allow manual trigger
    inputs:
        days_back:
            description: 'Days back to scrape'
            required: false
            default: '1'
            type: string

jobs:
  scrape-palm-beach:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb google-chrome-stable

      - name: Install Python Dependencies
        run: |
          pip install drissionpage gspread google-auth

      - name: Run PBSO Scraper
        env:
          GOOGLE_APPLICATION_CREDENTIALS_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY_PATH }}
        run: |
            # Decode secret
            echo "$GOOGLE_APPLICATION_CREDENTIALS_JSON" | base64 -d > service-account-key.json
            export GOOGLE_SERVICE_ACCOUNT_KEY_PATH=$(pwd)/service-account-key.json
            
            # Run with Xvfb (Virtual Framebuffer) to support "headed" mode in CI
            # -a: auto number, --server-args: screen config
            xvfb-run -a --server-args="-screen 0 1280x1024x24" python3 python_scrapers/scrapers/run_palm_beach.py ${{ inputs.days_back || '1' }}
